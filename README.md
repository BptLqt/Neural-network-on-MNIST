# Neural-network-on-MNIST
Simple change in the Softmax Regression algorithm by adding an extra layer.
This algorithm is like the Softmax Regression on MNIST one with just an extra layer combined with a ReLU activation function. We can see that adding a non-linear activation function we can now represent non-linearity with the model and thus, augment it precision.
