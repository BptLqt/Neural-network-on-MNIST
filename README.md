# Neural-network-on-MNIST
Simple change in the Softmax Regression algorithm by adding an extra layer.
This algorithm is like the Softmax Regression on MNIST one with just an extra layer combined with a ReLU activation function. We can see that adding a non-linear activation function we can now represent non-linearity with the model and thus, augment it precision.
With this algorithm, at the end of training we get a 0.97 accuracy on the test set, with 20 epochs less than the simple Softmax Regression.
